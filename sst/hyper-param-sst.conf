[hparam]
data = data/sst_data/raw_data/train_noex5_2017_09_13.pt
pre_word_vecs = data/sst_data/raw_data/wv_noex5_2017_09_13.npz
save_model = sst/data/
log = sst/log/
interval = 100

classes = 5


## Shared Memory
word_vec_size = 300
mem_size = 300
# LSTM
lstm = False
layer = 1
brnn = False
# CNN
kernel_size = 3,4,5
use_batch_norm = False


## Attention subnet
layer_c = 1
rnn_size_c = 300
hop = 2


# hyper-parameters
temp = 1.0
alpha = 1
beta = 2.5
enpy = -0.7


## Optimizition
batch_size = 32
epochs = 30
start_epoch = 1
param_init = 0.1
# rmsprop, sgd, adagrad, adadelta, adam, rmsprop
optim = rmsprop
max_grad_norm = 0.05
learning_rate = 0.0001

learning_rate_decay = 1
start_decay_at = 100

dropout = 0.5
extra_shuffle = False
weight_decay = 0


# visualize with tensorboardX
visualize = False










